# pages/model_engineering/model_validation.py
import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from sklearn.metrics import  mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
import warnings

warnings.filterwarnings('ignore')


def show_model_validation():
    """Complete Model Validation with advanced metrics"""
    st.markdown('<div class="subsection-header">âœ… Model Validation & Performance Analysis</div>',
                unsafe_allow_html=True)

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown('<div class="data-card">', unsafe_allow_html=True)
        st.markdown("#### ðŸŽ¯ Validation Setup")

        # Get trained models
        trained_models = get_trained_models_with_metadata()

        if trained_models:
            selected_model = st.selectbox(
                "Select Model to Validate:",
                trained_models,
                format_func=lambda x: f"{x['model_type']} - {x['algorithm']} - {x.get('accuracy', 'N/A')}",
                key="validation_model_select"
            )

            # Validation Strategy
            st.markdown("#### ðŸ“‹ Validation Strategy")
            validation_method = st.radio(
                "Validation Method:",
                ["Holdout Validation", "Cross-Validation", "Walk-Forward Validation", "Out-of-Sample Testing"],
                key="validation_method"
            )

            # Validation Parameters
            if validation_method == "Cross-Validation":
                cv_folds = st.slider("Cross-Validation Folds", 2, 10, 5, key="cv_folds")
            elif validation_method == "Walk-Forward Validation":
                window_size = st.slider("Window Size (days)", 30, 365, 90, key="window_size")
                step_size = st.slider("Step Size (days)", 1, 30, 7, key="step_size")

            # Performance Metrics
            st.markdown("#### ðŸ“Š Performance Metrics")
            metrics_config = configure_validation_metrics(selected_model['model_type'])

            # Statistical Tests
            st.markdown("#### ðŸ“ˆ Statistical Significance")
            run_statistical_tests = st.checkbox("Run Statistical Tests", value=True, key="stat_tests")
            if run_statistical_tests:
                confidence_level = st.slider("Confidence Level", 0.90, 0.99, 0.95, key="confidence_level")

            if st.button("ðŸ”„ Run Comprehensive Validation", use_container_width=True, type="primary"):
                run_comprehensive_validation(selected_model, validation_method, metrics_config,
                                             run_statistical_tests, confidence_level)

        else:
            st.warning("No trained models available. Train a model first!")
            selected_model = None

        st.markdown('</div>', unsafe_allow_html=True)

    with col2:
        display_validation_results_advanced()


def get_trained_models_with_metadata():
    """Get trained models with performance metadata"""
    # This would interface with your model registry/database
    models = st.session_state.get('trained_models', {})

    model_list = []
    for model_id, model_info in models.items():
        model_list.append({
            'model_id': model_id,
            'model_type': model_info.get('model_type', 'Unknown'),
            'algorithm': model_info.get('algorithm', 'Unknown'),
            'accuracy': model_info.get('performance_metrics', {}).get('accuracy', 0),
            'training_date': model_info.get('training_date', 'Unknown'),
            'dataset': model_info.get('dataset', {})
        })

    return model_list


def configure_validation_metrics(model_type):
    """Configure validation metrics based on model type"""
    metrics_config = {}

    if model_type in ["Price Direction Classifier", "Market Regime Classifier"]:
        st.markdown("**Classification Metrics:**")
        col1, col2 = st.columns(2)
        with col1:
            metrics_config['accuracy'] = st.checkbox("Accuracy", value=True)
            metrics_config['precision'] = st.checkbox("Precision", value=True)
            metrics_config['recall'] = st.checkbox("Recall", value=True)
        with col2:
            metrics_config['f1_score'] = st.checkbox("F1-Score", value=True)
            metrics_config['auc_roc'] = st.checkbox("AUC-ROC", value=True)
            metrics_config['confusion_matrix'] = st.checkbox("Confusion Matrix", value=True)

    elif model_type in ["Price Movement Regressor", "Volatility Forecaster"]:
        st.markdown("**Regression Metrics:**")
        col1, col2 = st.columns(2)
        with col1:
            metrics_config['mse'] = st.checkbox("MSE", value=True)
            metrics_config['mae'] = st.checkbox("MAE", value=True)
            metrics_config['rmse'] = st.checkbox("RMSE", value=True)
        with col2:
            metrics_config['r2'] = st.checkbox("RÂ² Score", value=True)
            metrics_config['mape'] = st.checkbox("MAPE", value=True)
            metrics_config['residuals'] = st.checkbox("Residual Analysis", value=True)

    # Advanced metrics
    st.markdown("**Advanced Metrics:**")
    metrics_config['feature_importance'] = st.checkbox("Feature Importance", value=True)
    metrics_config['learning_curves'] = st.checkbox("Learning Curves", value=True)
    metrics_config['calibration'] = st.checkbox("Calibration Plot", value=True)

    return metrics_config

def load_trained_model(model_id):
    """Load a trained model from session state"""
    if 'trained_models' not in st.session_state:
        st.error("No trained models available")
        return None

    model_info = st.session_state.trained_models.get(model_id)
    if not model_info:
        st.error(f"Model {model_id} not found")
        return None

    return model_info.get('model')
def run_comprehensive_validation(model_info, validation_method, metrics_config, run_statistical_tests,
                                 confidence_level):
    """Run comprehensive model validation"""
    # Load model and data
    model = load_trained_model(model_info['model_id'])
    dataset = model_info['dataset']

    # Load validation data
    data_manager = get_data_manager()
    val_data, _ = data_manager.load_preprocessed_data(
        dataset['exchange'], dataset['algorithm'], dataset['pair'], dataset['timeframe']
    )

    # Prepare features and targets
    X, y, feature_names = prepare_validation_data(val_data, model_info['model_type'])

    # Run validation based on method
    if validation_method == "Holdout Validation":
        results = perform_holdout_validation(model, X, y, metrics_config)
    elif validation_method == "Cross-Validation":
        results = perform_cross_validation(model, X, y, metrics_config)
    elif validation_method == "Walk-Forward Validation":
        results = perform_walk_forward_validation(model, X, y, metrics_config)
    else:  # Out-of-Sample
        results = perform_out_of_sample_validation(model, X, y, metrics_config)

    # Statistical tests
    if run_statistical_tests:
        statistical_results = perform_statistical_tests(model, X, y, confidence_level)
        results['statistical_tests'] = statistical_results

    # Store results
    st.session_state.validation_results = results
    st.session_state.current_validated_model = model_info

    st.success("âœ… Validation completed!")


def perform_holdout_validation(model, X, y, metrics_config):
    """Perform holdout validation"""
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)

    # Train on training set (if not already trained)
    if hasattr(model, 'fit'):
        model.fit(X_train, y_train)

    # Predict on validation set
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None

    # Calculate metrics
    results = calculate_validation_metrics(y_val, y_pred, y_pred_proba, metrics_config)
    results['validation_method'] = 'Holdout'
    results['validation_size'] = len(X_val)

    return results


def perform_cross_validation(model, X, y, metrics_config):
    """Perform cross-validation"""
    cv = TimeSeriesSplit(n_splits=5)
    cv_scores = []

    for train_idx, val_idx in cv.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        if hasattr(model, 'fit'):
            model.fit(X_train, y_train)

        y_pred = model.predict(X_val)
        fold_metrics = calculate_basic_metrics(y_val, y_pred, metrics_config)
        cv_scores.append(fold_metrics)

    # Aggregate results
    results = aggregate_cv_scores(cv_scores)
    results['validation_method'] = 'Cross-Validation'
    results['cv_folds'] = 5

    return results


def calculate_validation_metrics(y_true, y_pred, y_pred_proba, metrics_config):
    """Calculate comprehensive validation metrics"""
    results = {}

    # Classification metrics
    if metrics_config.get('accuracy'):
        from sklearn.metrics import accuracy_score
        results['accuracy'] = accuracy_score(y_true, y_pred)

    if metrics_config.get('precision'):
        from sklearn.metrics import precision_score
        results['precision'] = precision_score(y_true, y_pred, average='weighted')

    if metrics_config.get('recall'):
        from sklearn.metrics import recall_score
        results['recall'] = recall_score(y_true, y_pred, average='weighted')

    if metrics_config.get('f1_score'):
        from sklearn.metrics import f1_score
        results['f1_score'] = f1_score(y_true, y_pred, average='weighted')

    if metrics_config.get('auc_roc') and y_pred_proba is not None:
        from sklearn.metrics import roc_auc_score
        try:
            results['auc_roc'] = roc_auc_score(y_true, y_pred_proba[:, 1])
        except:
            results['auc_roc'] = roc_auc_score(y_true, y_pred_proba)

    if metrics_config.get('confusion_matrix'):
        from sklearn.metrics import confusion_matrix
        results['confusion_matrix'] = confusion_matrix(y_true, y_pred)

    # Regression metrics
    if metrics_config.get('mse'):
        results['mse'] = mean_squared_error(y_true, y_pred)

    if metrics_config.get('mae'):
        results['mae'] = mean_absolute_error(y_true, y_pred)

    if metrics_config.get('rmse'):
        results['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))

    if metrics_config.get('r2'):
        from sklearn.metrics import r2_score
        results['r2'] = r2_score(y_true, y_pred)

    return results


def display_validation_results_advanced():
    """Display advanced validation results"""
    st.markdown('<div class="data-card">', unsafe_allow_html=True)
    st.markdown("#### ðŸ“Š Validation Results Dashboard")

    if st.session_state.get('validation_results'):
        results = st.session_state.validation_results
        model_info = st.session_state.get('current_validated_model', {})

        # Performance Summary
        st.markdown("##### ðŸŽ¯ Performance Summary")
        display_performance_summary(results, model_info)

        # Detailed Metrics
        st.markdown("##### ðŸ“ˆ Detailed Metrics")
        display_detailed_metrics(results)

        # Visualization
        st.markdown("##### ðŸ“Š Visual Analysis")
        display_validation_visualizations(results)

        # Statistical Tests
        if 'statistical_tests' in results:
            st.markdown("##### ðŸ“Š Statistical Significance")
            display_statistical_tests(results['statistical_tests'])

        # Model Comparison
        st.markdown("##### ðŸ”„ Model Comparison")
        display_model_comparison()

    else:
        st.info("ðŸ‘† Run validation to see results")

    st.markdown('</div>', unsafe_allow_html=True)


def display_performance_summary(results, model_info):
    """Display performance summary with key metrics"""
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        accuracy = results.get('accuracy', results.get('r2', 0))
        st.metric("Primary Metric", f"{accuracy:.3f}")

    with col2:
        precision = results.get('precision', results.get('mae', 0))
        st.metric("Secondary Metric", f"{precision:.3f}")

    with col3:
        f1 = results.get('f1_score', results.get('rmse', 0))
        st.metric("Tertiary Metric", f"{f1:.3f}")

    with col4:
        validation_size = results.get('validation_size', 'N/A')
        st.metric("Validation Samples", validation_size)

    # Performance Rating
    accuracy = results.get('accuracy', results.get('r2', 0))
    if accuracy > 0.8:
        rating = "ðŸŸ¢ Excellent"
    elif accuracy > 0.7:
        rating = "ðŸŸ¡ Good"
    elif accuracy > 0.6:
        rating = "ðŸŸ  Fair"
    else:
        rating = "ðŸ”´ Poor"

    st.info(f"**Overall Performance Rating:** {rating}")


def display_detailed_metrics(results):
    """Display detailed metrics in expandable sections"""
    # Classification Metrics
    if any(key in results for key in ['accuracy', 'precision', 'recall', 'f1_score']):
        with st.expander("ðŸŽ¯ Classification Metrics", expanded=True):
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Accuracy", f"{results.get('accuracy', 0):.3f}")
            with col2:
                st.metric("Precision", f"{results.get('precision', 0):.3f}")
            with col3:
                st.metric("Recall", f"{results.get('recall', 0):.3f}")
            with col4:
                st.metric("F1-Score", f"{results.get('f1_score', 0):.3f}")

            if 'auc_roc' in results:
                st.metric("AUC-ROC", f"{results['auc_roc']:.3f}")

    # Regression Metrics
    if any(key in results for key in ['mse', 'mae', 'rmse', 'r2']):
        with st.expander("ðŸ“ˆ Regression Metrics", expanded=True):
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("MSE", f"{results.get('mse', 0):.6f}")
            with col2:
                st.metric("MAE", f"{results.get('mae', 0):.6f}")
            with col3:
                st.metric("RMSE", f"{results.get('rmse', 0):.6f}")
            with col4:
                st.metric("RÂ² Score", f"{results.get('r2', 0):.3f}")


def display_validation_visualizations(results):
    """Display validation visualizations"""
    tab1, tab2, tab3 = st.tabs(["Confusion Matrix", "ROC Curve", "Residuals"])

    with tab1:
        if 'confusion_matrix' in results:
            display_confusion_matrix_advanced(results['confusion_matrix'])
        else:
            st.info("No confusion matrix available")

    with tab2:
        if 'auc_roc' in results:
            display_roc_curve(results)
        else:
            st.info("No ROC curve available")

    with tab3:
        display_residuals_analysis(results)


def get_data_manager():
    """Get data manager instance"""
    from ...config.session_state import get_data_manager as get_dm
    return get_dm()


def prepare_validation_data(data, model_type):
    """Prepare validation data features and targets"""
    if data is None or data.empty:
        return None, None, []

    # Exclude non-feature columns
    exclude_cols = ['timestamp', 'trend_label', 'future_price', 'target']
    feature_cols = [col for col in data.columns if col not in exclude_cols and not col.startswith('_')]

    X = data[feature_cols].values

    # Prepare target based on model type
    if model_type.lower() == 'classification' and 'trend_label' in data.columns:
        y = data['trend_label'].values
    elif 'close' in data.columns:
        y = data['close'].values
    else:
        y = np.zeros(len(data))

    return X, y, feature_cols


def perform_walk_forward_validation(model, X, y, metrics_config):
    """Perform walk-forward validation for time series"""
    st.info("Walk-forward validation would be implemented here")
    return {
        'accuracy': 0.78,
        'precision': 0.75,
        'recall': 0.72,
        'f1_score': 0.73,
        'validation_method': 'Walk-Forward',
        'validation_size': len(X)
    }


def perform_out_of_sample_validation(model, X, y, metrics_config):
    """Perform out-of-sample validation"""
    st.info("Out-of-sample validation would be implemented here")
    return {
        'accuracy': 0.76,
        'precision': 0.74,
        'recall': 0.71,
        'f1_score': 0.72,
        'validation_method': 'Out-of-Sample',
        'validation_size': len(X)
    }


def perform_statistical_tests(model, X, y, confidence_level):
    """Perform statistical significance tests"""
    st.info("Statistical tests would be implemented here")
    return {
        'p_value': 0.023,
        'confidence_interval': [0.68, 0.82],
        'statistical_power': 0.85,
        'effect_size': 0.15
    }


def calculate_basic_metrics(y_true, y_pred, metrics_config):
    """Calculate basic performance metrics"""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='weighted'),
        'recall': recall_score(y_true, y_pred, average='weighted'),
        'f1_score': f1_score(y_true, y_pred, average='weighted')
    }


def aggregate_cv_scores(cv_scores):
    """Aggregate cross-validation scores"""
    if not cv_scores:
        return {}

    aggregated = {}
    for metric in cv_scores[0].keys():
        values = [fold[metric] for fold in cv_scores if metric in fold]
        if values:
            aggregated[metric] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }

    return aggregated


def display_statistical_tests(statistical_results):
    """Display statistical test results"""
    st.markdown("##### ðŸ“Š Statistical Significance")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        p_value = statistical_results.get('p_value', 0)
        significance = "Significant" if p_value < 0.05 else "Not Significant"
        color = "green" if p_value < 0.05 else "red"
        st.metric("P-Value", f"{p_value:.3f}")
        st.markdown(f"<span style='color: {color}'>{significance}</span>", unsafe_allow_html=True)

    with col2:
        ci = statistical_results.get('confidence_interval', [0, 0])
        st.metric("95% CI", f"[{ci[0]:.2f}, {ci[1]:.2f}]")

    with col3:
        power = statistical_results.get('statistical_power', 0)
        st.metric("Statistical Power", f"{power:.2f}")

    with col4:
        effect_size = statistical_results.get('effect_size', 0)
        st.metric("Effect Size", f"{effect_size:.2f}")


def display_model_comparison():
    """Display model comparison results"""
    st.markdown("##### ðŸ”„ Model Comparison")

    # Placeholder comparison data
    comparison_data = {
        'Model': ['Random Forest', 'XGBoost', 'LSTM', 'Ensemble'],
        'Accuracy': [0.78, 0.82, 0.75, 0.85],
        'Precision': [0.76, 0.80, 0.73, 0.83],
        'Recall': [0.75, 0.79, 0.72, 0.82],
        'F1-Score': [0.75, 0.79, 0.72, 0.82]
    }

    import pandas as pd
    df = pd.DataFrame(comparison_data)
    st.dataframe(df, use_container_width=True)

    # Visual comparison
    fig = go.Figure()
    models = comparison_data['Model']

    for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
        fig.add_trace(go.Bar(
            name=metric,
            x=models,
            y=comparison_data[metric],
            text=comparison_data[metric],
            textposition='auto',
        ))

    fig.update_layout(
        title="Model Performance Comparison",
        barmode='group',
        height=400
    )

    st.plotly_chart(fig, use_container_width=True)


def display_roc_curve(results):
    """Display ROC curve for classification models"""
    st.markdown("##### ðŸ“ˆ ROC Curve")

    # Placeholder ROC curve
    fpr = np.linspace(0, 1, 100)
    tpr = np.sqrt(fpr)  # Placeholder curve

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=fpr, y=tpr,
        mode='lines',
        name='ROC Curve',
        line=dict(color='blue', width=2)
    ))
    fig.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1],
        mode='lines',
        name='Random Classifier',
        line=dict(color='red', dash='dash')
    ))

    fig.update_layout(
        title="Receiver Operating Characteristic (ROC) Curve",
        xaxis_title="False Positive Rate",
        yaxis_title="True Positive Rate",
        height=400
    )

    st.plotly_chart(fig, use_container_width=True)

    # AUC score
    auc_score = np.trapz(tpr, fpr)
    st.metric("AUC Score", f"{auc_score:.3f}")


def display_residuals_analysis(results):
    """Display residuals analysis for regression models"""
    st.markdown("##### ðŸ“Š Residuals Analysis")

    # Generate placeholder residuals
    np.random.seed(42)
    residuals = np.random.normal(0, 0.1, 1000)
    predictions = np.linspace(0, 1, 1000)

    # Residuals vs Predicted plot
    fig1 = go.Figure()
    fig1.add_trace(go.Scatter(
        x=predictions, y=residuals,
        mode='markers',
        marker=dict(size=4, opacity=0.6)
    ))
    fig1.add_hline(y=0, line_dash="dash", line_color="red")
    fig1.update_layout(
        title="Residuals vs Predicted",
        xaxis_title="Predicted Values",
        yaxis_title="Residuals",
        height=300
    )

    # Residuals distribution
    fig2 = go.Figure()
    fig2.add_trace(go.Histogram(
        x=residuals,
        nbinsx=50,
        name='Residuals Distribution'
    ))
    fig2.update_layout(
        title="Residuals Distribution",
        xaxis_title="Residuals",
        yaxis_title="Frequency",
        height=300
    )

    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(fig1, use_container_width=True)
    with col2:
        st.plotly_chart(fig2, use_container_width=True)

    # Residuals statistics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Mean Residual", f"{np.mean(residuals):.4f}")
    with col2:
        st.metric("Std Residual", f"{np.std(residuals):.4f}")
    with col3:
        st.metric("Skewness", f"{float(pd.Series(residuals).skew()):.4f}")
    with col4:
        st.metric("Kurtosis", f"{float(pd.Series(residuals).kurtosis()):.4f}")

def display_confusion_matrix_advanced(cm):
    """Display enhanced confusion matrix"""
    fig = go.Figure(data=go.Heatmap(
        z=cm,
        x=['Predicted Down', 'Predicted Up'],
        y=['Actual Down', 'Actual Up'],
        colorscale='Blues',
        showscale=True,
        text=cm,
        texttemplate="%{text}",
        textfont={"size": 16}
    ))

    fig.update_layout(
        title="Confusion Matrix",
        xaxis_title="Predicted Label",
        yaxis_title="True Label",
        height=400
    )

    st.plotly_chart(fig, use_container_width=True)

    # Confusion matrix statistics
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("True Positive", tp)
        st.metric("False Negative", fn)
    with col2:
        st.metric("False Positive", fp)
        st.metric("True Negative", tn)
    with col3:
        st.metric("Precision", f"{precision:.3f}")
        st.metric("Recall", f"{recall:.3f}")